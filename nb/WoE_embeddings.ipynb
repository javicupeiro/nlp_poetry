{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = \"../data/glove/glove.6B.300d.txt\"\n",
    "glove = KeyedVectors.load_word2vec_format(fname=fname,\n",
    "                                          no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size-> words: 400000, dimensions: 300\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embedding size-> words: {glove.vectors.shape[0]}, dimensions: {glove.vectors.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Similarity\n",
    "We can check the 10 most similar words using ``most_similar``, it uses cosine similarity to check which embeddings are more similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cacti', 0.6634564399719238),\n",
       " ('saguaro', 0.6195855140686035),\n",
       " ('pear', 0.5233486890792847),\n",
       " ('cactuses', 0.5178281664848328),\n",
       " ('prickly', 0.515631914138794),\n",
       " ('mesquite', 0.4844855070114136),\n",
       " ('opuntia', 0.4540084898471832),\n",
       " ('shrubs', 0.45362064242362976),\n",
       " ('peyote', 0.45344963669776917),\n",
       " ('succulents', 0.4512787461280823)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.most_similar(\"cactus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('falling', 0.6513392925262451),\n",
       " ('rise', 0.6301450729370117),\n",
       " ('drop', 0.6298140287399292),\n",
       " ('decline', 0.6145920157432556),\n",
       " ('beginning', 0.6086390614509583),\n",
       " ('spring', 0.5864909887313843),\n",
       " ('year', 0.5789673328399658),\n",
       " ('coming', 0.5778051018714905),\n",
       " ('fallen', 0.5676990747451782),\n",
       " ('fell', 0.5675972104072571)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see different meanings for 'fall'--> falling != spring\n",
    "glove.most_similar(\"fall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Word Analogies\n",
    "\n",
    "We will check how semantic information is encoded by word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6713276505470276),\n",
       " ('princess', 0.5432624220848083),\n",
       " ('throne', 0.5386104583740234),\n",
       " ('monarch', 0.5347574949264526),\n",
       " ('daughter', 0.498025119304657),\n",
       " ('mother', 0.4956442713737488),\n",
       " ('elizabeth', 0.483265221118927),\n",
       " ('kingdom', 0.47747090458869934),\n",
       " ('prince', 0.4668239951133728),\n",
       " ('wife', 0.46473270654678345)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out = king - man + woman\n",
    "glove.most_similar(positive=[\"king\", \"woman\"],\n",
    "                   negative=[\"man\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mexico', 0.5726832151412964),\n",
       " ('philippines', 0.5445368885993958),\n",
       " ('peru', 0.4838225543498993),\n",
       " ('venezuela', 0.4816672205924988),\n",
       " ('brazil', 0.4664309620857239),\n",
       " ('argentina', 0.45490506291389465),\n",
       " ('philippine', 0.4417841136455536),\n",
       " ('chile', 0.4396097660064697),\n",
       " ('colombia', 0.4386259913444519),\n",
       " ('thailand', 0.43396785855293274)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out = japan - yen + peso\n",
    "glove.most_similar(positive=[\"japan\", \"peso\"],\n",
    "                   negative=[\"yen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('venezuela', 0.5744216442108154),\n",
       " ('nicaragua', 0.54659104347229),\n",
       " ('cuban', 0.5447268486022949),\n",
       " ('mexico', 0.5030182600021362),\n",
       " ('dominican', 0.4905185103416443),\n",
       " ('castro', 0.47028154134750366),\n",
       " ('argentina', 0.4679957926273346),\n",
       " ('panama', 0.45990291237831116),\n",
       " ('honduras', 0.4594337046146393),\n",
       " ('cubans', 0.45838162302970886)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out = spain - madrid + cuba\n",
    "glove.most_similar(positive=[\"spain\", \"cuba\"],\n",
    "                   negative=[\"madrid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tallest', 0.5077418684959412),\n",
       " ('taller', 0.47616496682167053),\n",
       " ('height', 0.46000051498413086),\n",
       " ('metres', 0.4584786593914032),\n",
       " ('cm', 0.45212721824645996),\n",
       " ('meters', 0.44067245721817017),\n",
       " ('towering', 0.42784255743026733),\n",
       " ('centimeters', 0.42345431447029114),\n",
       " ('inches', 0.4174586832523346),\n",
       " ('erect', 0.4087314009666443)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out = best - good + tall\n",
    "glove.most_similar(positive=[\"best\", \"tall\"],\n",
    "                   negative=[\"good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('largest', 0.5376060605049133),\n",
       " ('tiny', 0.5351578593254089),\n",
       " ('large', 0.5282967686653137),\n",
       " ('smallest', 0.50852370262146),\n",
       " ('smaller', 0.5056758522987366),\n",
       " ('larger', 0.4700247049331665),\n",
       " ('scale', 0.43181347846984863),\n",
       " ('sized', 0.4149516820907593),\n",
       " ('in', 0.40775397419929504),\n",
       " ('biggest', 0.406604140996933)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out = worst - bad + small\n",
    "glove.most_similar(positive=[\"worst\", \"small\"],\n",
    "                   negative=[\"bad\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Custom Word Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_words(word, vectors, index2key, key2index, topn=10):\n",
    "    word_id = key2index[word]\n",
    "    emb = vectors[word_id]\n",
    "    # Calculate similarities to all words in the vocabulary\n",
    "    similarities = vectors @ emb\n",
    "    # We sort words that are similar in ascending order\n",
    "    ids_ascending = similarities.argsort()\n",
    "    ids_descending = ids_ascending[::-1] # same list but descending order\n",
    "    # get boolean array with element corresponding to word_id set to false\n",
    "    mask = ids_descending != word_id\n",
    "    # obtain new array of indices that doesn't contain word_id\n",
    "    # (otherwise the most similar word to the argument would be the argument itself)\n",
    "    ids_descending = ids_descending[mask]\n",
    "    # get topn word_ids\n",
    "    top_ids = ids_descending[:topn]\n",
    "    # retrieve topn words with their corresponding similarity score\n",
    "    top_words = [(index2key[i], similarities[i]) for i in top_ids]\n",
    "    # return results\n",
    "    return top_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cacti', 0.66345644),\n",
       " ('saguaro', 0.6195854),\n",
       " ('pear', 0.5233487),\n",
       " ('cactuses', 0.5178282),\n",
       " ('prickly', 0.5156319),\n",
       " ('mesquite', 0.4844855),\n",
       " ('opuntia', 0.4540084),\n",
       " ('shrubs', 0.45362067),\n",
       " ('peyote', 0.4534496),\n",
       " ('succulents', 0.45127875)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectors = glove.get_normed_vectors()\n",
    "index_to_key = glove.index_to_key\n",
    "key_to_index = glove.key_to_index\n",
    "most_similar_words(\"cactus\", vectors, index_to_key, key_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cacti', 0.6634564399719238),\n",
       " ('saguaro', 0.6195855140686035),\n",
       " ('pear', 0.5233486890792847),\n",
       " ('cactuses', 0.5178281664848328),\n",
       " ('prickly', 0.515631914138794),\n",
       " ('mesquite', 0.4844855070114136),\n",
       " ('opuntia', 0.4540084898471832),\n",
       " ('shrubs', 0.45362064242362976),\n",
       " ('peyote', 0.45344963669776917),\n",
       " ('succulents', 0.4512787461280823)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check with \n",
    "glove.most_similar(\"cactus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Custom Analogy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.linalg import norm\n",
    "\n",
    "def analogy(positive, negative, vectors, index_to_key, key_to_index, topn=10):\n",
    "    # find ids for positive and negative words\n",
    "    pos_ids = [key_to_index[w] for w in positive]\n",
    "    neg_ids = [key_to_index[w] for w in negative]\n",
    "    given_word_ids = pos_ids + neg_ids\n",
    "    # get embeddings for positive and negative words\n",
    "    pos_emb = vectors[pos_ids].sum(axis=0)\n",
    "    neg_emb = vectors[neg_ids].sum(axis=0)\n",
    "    # get embedding for analogy\n",
    "    emb = pos_emb - neg_emb\n",
    "    # normalize embedding\n",
    "    emb = emb / norm(emb)\n",
    "    # calculate similarities to all words in out vocabulary\n",
    "    similarities = vectors @ emb\n",
    "    # get word_ids in ascending order with respect to similarity score\n",
    "    ids_ascending = similarities.argsort()\n",
    "    # reverse word_ids\n",
    "    ids_descending = ids_ascending[::-1]\n",
    "    # get boolean array with element corresponding to any of given_word_ids set to false\n",
    "    given_words_mask = np.isin(ids_descending, given_word_ids, invert=True)\n",
    "    # obtain new array of indices that doesn't contain any of the given_word_ids\n",
    "    ids_descending = ids_descending[given_words_mask]\n",
    "    # get topn word_ids\n",
    "    top_ids = ids_descending[:topn]\n",
    "    # retrieve topn words with their corresponding similarity score\n",
    "    top_words = [(index_to_key[i], similarities[i]) for i in top_ids]\n",
    "    # return results\n",
    "    return top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6713277),\n",
       " ('princess', 0.5432624),\n",
       " ('throne', 0.5386105),\n",
       " ('monarch', 0.5347575),\n",
       " ('daughter', 0.49802512),\n",
       " ('mother', 0.49564424),\n",
       " ('elizabeth', 0.48326525),\n",
       " ('kingdom', 0.47747087),\n",
       " ('prince', 0.466824),\n",
       " ('wife', 0.46473265)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive = [\"king\", \"woman\"]\n",
    "negative = [\"man\"]\n",
    "vectors = glove.get_normed_vectors()\n",
    "index_to_key = glove.index_to_key\n",
    "key_to_index = glove.key_to_index\n",
    "analogy(positive, negative, vectors, index_to_key, key_to_index)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
