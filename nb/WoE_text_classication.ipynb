{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(400000, 300)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fname = \"../data/glove.6B.300d.txt\"\n",
    "glove = KeyedVectors.load_word2vec_format(fname=fname,\n",
    "                                          no_header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding size-> words: 400000, dimensions: 300\n"
     ]
    }
   ],
   "source": [
    "print(f\"Embedding size-> words: {glove.vectors.shape[0]}, dimensions: {glove.vectors.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Similarity\n",
    "We can check the 10 most similar words using ``most_similar``, it uses cosine similarity to check which embeddings are more similar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('cacti', 0.6634564399719238),\n",
       " ('saguaro', 0.6195855140686035),\n",
       " ('pear', 0.5233486890792847),\n",
       " ('cactuses', 0.5178281664848328),\n",
       " ('prickly', 0.515631914138794),\n",
       " ('mesquite', 0.4844855070114136),\n",
       " ('opuntia', 0.4540084898471832),\n",
       " ('shrubs', 0.45362064242362976),\n",
       " ('peyote', 0.45344963669776917),\n",
       " ('succulents', 0.4512787461280823)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.most_similar(\"cactus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('falling', 0.6513392925262451),\n",
       " ('rise', 0.6301450729370117),\n",
       " ('drop', 0.6298140287399292),\n",
       " ('decline', 0.6145920157432556),\n",
       " ('beginning', 0.6086390614509583),\n",
       " ('spring', 0.5864909887313843),\n",
       " ('year', 0.5789673328399658),\n",
       " ('coming', 0.5778051018714905),\n",
       " ('fallen', 0.5676990747451782),\n",
       " ('fell', 0.5675972104072571)]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see different meanings for 'fall'--> falling != spring\n",
    "glove.most_similar(\"fall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Word Analogies\n",
    "\n",
    "We will check how semantic information is encoded by word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.6713276505470276),\n",
       " ('princess', 0.5432624220848083),\n",
       " ('throne', 0.5386104583740234),\n",
       " ('monarch', 0.5347574949264526),\n",
       " ('daughter', 0.498025119304657),\n",
       " ('mother', 0.4956442713737488),\n",
       " ('elizabeth', 0.483265221118927),\n",
       " ('kingdom', 0.47747090458869934),\n",
       " ('prince', 0.4668239951133728),\n",
       " ('wife', 0.46473270654678345)]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out = king - man + woman\n",
    "glove.most_similar(positive=[\"king\", \"woman\"],\n",
    "                   negative=[\"man\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mexico', 0.5726832151412964),\n",
       " ('philippines', 0.5445368885993958),\n",
       " ('peru', 0.4838225543498993),\n",
       " ('venezuela', 0.4816672205924988),\n",
       " ('brazil', 0.4664309620857239),\n",
       " ('argentina', 0.45490506291389465),\n",
       " ('philippine', 0.4417841136455536),\n",
       " ('chile', 0.4396097660064697),\n",
       " ('colombia', 0.4386259913444519),\n",
       " ('thailand', 0.43396785855293274)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out = japan - yen + peso\n",
    "glove.most_similar(positive=[\"japan\", \"peso\"],\n",
    "                   negative=[\"yen\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('venezuela', 0.5744216442108154),\n",
       " ('nicaragua', 0.54659104347229),\n",
       " ('cuban', 0.5447268486022949),\n",
       " ('mexico', 0.5030182600021362),\n",
       " ('dominican', 0.4905185103416443),\n",
       " ('castro', 0.47028154134750366),\n",
       " ('argentina', 0.4679957926273346),\n",
       " ('panama', 0.45990291237831116),\n",
       " ('honduras', 0.4594337046146393),\n",
       " ('cubans', 0.45838162302970886)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out = spain - madrid + cuba\n",
    "glove.most_similar(positive=[\"spain\", \"cuba\"],\n",
    "                   negative=[\"madrid\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tallest', 0.5077418684959412),\n",
       " ('taller', 0.47616496682167053),\n",
       " ('height', 0.46000051498413086),\n",
       " ('metres', 0.4584786593914032),\n",
       " ('cm', 0.45212721824645996),\n",
       " ('meters', 0.44067245721817017),\n",
       " ('towering', 0.42784255743026733),\n",
       " ('centimeters', 0.42345431447029114),\n",
       " ('inches', 0.4174586832523346),\n",
       " ('erect', 0.4087314009666443)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out = best - good + tall\n",
    "glove.most_similar(positive=[\"best\", \"tall\"],\n",
    "                   negative=[\"good\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('largest', 0.5376060605049133),\n",
       " ('tiny', 0.5351578593254089),\n",
       " ('large', 0.5282967686653137),\n",
       " ('smallest', 0.50852370262146),\n",
       " ('smaller', 0.5056758522987366),\n",
       " ('larger', 0.4700247049331665),\n",
       " ('scale', 0.43181347846984863),\n",
       " ('sized', 0.4149516820907593),\n",
       " ('in', 0.40775397419929504),\n",
       " ('biggest', 0.406604140996933)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# out = worst - bad + small\n",
    "glove.most_similar(positive=[\"worst\", \"small\"],\n",
    "                   negative=[\"bad\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
